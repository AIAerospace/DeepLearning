{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construyendo tu Red Neuronal Profunda: Paso a Paso\n",
    "\n",
    "En este cuaderno, implementarás todas las funciones necesarias para construir una red neuronal profunda.\n",
    "\n",
    "\n",
    "**Después de este cuaderno, serás capaz de:**\n",
    "- Usar unidades no lineales como ReLU para mejorar tu modelo.\n",
    "- Construir una red neuronal más profunda (con más de una capa oculta).\n",
    "- Implementar una clase de red neuronal fácil de usar.\n",
    "\n",
    "**Notación**:\n",
    "- El superíndice $[l]$ denota una cantidad asociada con la $l^{ésima}$ capa.  \n",
    "    - Ejemplo: $a^{[L]}$ es la activación de la capa $L$. $W^{[L]}$ y $b^{[L]}$ son los parámetros de la capa $L$.\n",
    "- El superíndice $(i)$ denota una cantidad asociada con el $i^{ésimo}$ ejemplo.  \n",
    "    - Ejemplo: $x^{(i)}$ es el $i^{ésimo}$ ejemplo de entrenamiento.\n",
    "- El subíndice $i$ denota la $i^{ésima}$ entrada de un vector.  \n",
    "    - Ejemplo: $a^{[l]}_i$ denota la $i^{ésima}$ entrada de las activaciones de la capa $l$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Paquetes  \n",
    "\n",
    "Primero, importemos todos los paquetes necesarios.  \n",
    "\n",
    "- [numpy](www.numpy.org) es el paquete principal para la computación científica con Python.  \n",
    "- [matplotlib](http://matplotlib.org) es una biblioteca para graficos en Python.  \n",
    "- `dnn_utils` proporciona algunas funciones necesarias para este cuaderno.  \n",
    "- `testCases` proporciona algunos casos de prueba para evaluar la corrección de tus funciones.  \n",
    "- `np.random.seed(1)` se usa para mantener consistentes todas las llamadas a funciones aleatorias. Esto nos ayudará a calificar tu trabajo. Por favor, no cambies la semilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v2 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - Esquema del cuaderno \n",
    "\n",
    "Para construir tu red neuronal, implementarás varias \"funciones auxiliares\". Estas funciones auxiliares se utilizarán en la próxima asignación para construir una red neuronal de dos capas y una red neuronal de L capas. Cada pequeña función auxiliar que implementarás tendrá instrucciones detalladas que te guiarán a través de los pasos necesarios. Aquí tienes un esquema de esta asignación, en la que harás lo siguiente:  \n",
    "\n",
    "- Inicializar los parámetros para una red neuronal de dos capas y para una red neuronal de L capas.  \n",
    "- Implementar el módulo de propagación hacia adelante.  \n",
    "     - Completar la parte **LINEAL** de la propagación hacia adelante de una capa (resultando en $ Z^{[l]} $).  \n",
    "     - Se te proporcionará la función de **ACTIVACIÓN** (ReLU/Sigmoide).  \n",
    "     - Combinar los dos pasos anteriores en una nueva función de propagación hacia adelante **[LINEAR->ACTIVATION]**.  \n",
    "     - Apilar la función de propagación hacia adelante **[LINEAR->RELU]** L-1  veces (para las capas 1 a  L-1 ) y agregar **[LINEAR->SIGMOID]** al final (para la capa final L ). Esto te dará una nueva función **L_model_forward**.  \n",
    "- Calcular la función de pérdida (loss).  \n",
    "- Implementar el módulo de propagación hacia atrás (indicado en rojo en la figura a continuación).  \n",
    "    - Completar la parte **LINEAL** de la propagación hacia atrás de una capa.  \n",
    "    - Se te proporcionará el gradiente de la función **ACTIVACIÓN** (relu_backward/sigmoid_backward).  \n",
    "    - Combinar los dos pasos anteriores en una nueva función de propagación hacia atrás **[LINEAR->ACTIVATION]**.  \n",
    "    - Apilar **[LINEAR->RELU]** hacia atrás L-1 veces y agregar **[LINEAR->SIGMOID]** hacia atrás en una nueva función **L_model_backward**.  \n",
    "- Finalmente, actualizar los parámetros.  \n",
    "\n",
    "\n",
    "**Nota:** Para cada función de propagación hacia adelante, existe una función correspondiente de propagación hacia atrás. Por ello, en cada paso del módulo de propagación hacia adelante almacenarás algunos valores en una memoria intermedia (**cache**). Estos valores almacenados serán útiles para calcular los gradientes. Luego, en el módulo de retropropagación, utilizarás la memoria intermedia para calcular los gradientes. Esta asignación te mostrará exactamente cómo llevar a cabo cada uno de estos pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 - Inicializar  \n",
    "\n",
    "La inicialización para una red neuronal profunda de $L$ capas es más compleja porque hay muchas más matrices de pesos y vectores de sesgo. Al completar `initialize_parameters_deep`, debes asegurarte de que las dimensiones coincidan entre cada capa. Recuerda que $ n^{[l]} $ es el número de unidades en la capa $l$. Por ejemplo, si el tamaño de nuestra entrada $X$ es $(12288, 209)$ (con $m = 209$ ejemplos), entonces:\n",
    "\n",
    "| **Capa** | **Forma de $ W $** | **Forma de $ b $** | **Fórmula de Activación** |\n",
    "|----------|---------------------|---------------------|-----------------------------|\n",
    "| 1        | $ (n^{[1]}, 12288) $ | $ (n^{[1]}, 1) $ | $ Z^{[1]} = W^{[1]} X + b^{[1]} $ |\n",
    "| 2        | $ (n^{[2]}, n^{[1]}) $ | $ (n^{[2]},1) $ | $ Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]} $ |\n",
    "| $ L-1 $ | $ (n^{[L-1]}, n^{[L-2]}) $ | $ (n^{[L-1]}, 1) $ | $ Z^{[L-1]} = W^{[L-1]} A^{[L-2]} + b^{[L-1]} $ |\n",
    "| $ L $ | $ (n^{[L]}, n^{[L-1]}) $ | $ (n^{[L]}, 1) $ | $ Z^{[L]} = W^{[L]} A^{[L-1]} + b^{[L]} $ |\n",
    "\n",
    "Recuerda que cuando computamos $ WX + b $ en Python, se realiza automáticamente la difusión de valores (**broadcasting**). Por ejemplo, si:\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n  & o \\\\  \n",
    "    p  & q  & r  \n",
    "\\end{bmatrix}, \\quad \n",
    "X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e  & f \\\\  \n",
    "    g  & h  & i  \n",
    "\\end{bmatrix}, \\quad  \n",
    "b =\\begin{bmatrix}\n",
    "    s  \\\\  \n",
    "    t  \\\\  \n",
    "    u  \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Entonces, $ WX + b $ será:\n",
    "\n",
    "$$\n",
    "WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio**: Implementar la inicialización para una Red Neuronal de $ L $ capas.  \n",
    "\n",
    "### **Instrucciones**:  \n",
    "- La estructura del modelo es *[LINEAR -> RELU] $ \\times (L-1) $ -> LINEAR -> SIGMOIDE*. Es decir, tiene $ L-1 $ capas con una función de activación **ReLU**, seguidas de una capa de salida con activación **sigmoide**.  \n",
    "- Usa inicialización aleatoria para las matrices de pesos. Utiliza `np.random.rand(shape) * 0.01`.  \n",
    "- Usa inicialización en ceros para los sesgos. Utiliza `np.zeros(shape)`.  \n",
    "- Almacenaremos $ n^{[l]} $, el número de unidades en las diferentes capas, en una variable llamada `layer_dims`.  \n",
    "  - Por ejemplo, si `layer_dims` fuera `[2,4,1]`. Esto significa:  \n",
    "    - **2 entradas**  \n",
    "    - **1 capa oculta con 4 unidades ocultas**  \n",
    "    - **1 capa de salida con 1 unidad de salida**  \n",
    "  - Como resultado, las dimensiones de los pesos y sesgos fueron:  \n",
    "    - `W1`: $ (4,2) $  \n",
    "    - `b1`: $ (4,1) $  \n",
    "    - `W2`: $ (1,4) $  \n",
    "    - `b2`: $ (1,1) $  \n",
    "  - Ahora, generalizaremos esto para $ L $ capas.  \n",
    "\n",
    "- A continuación, se muestra la implementación para $ L=1 $ (una red neuronal de una sola capa). Usa esto como referencia para implementar el caso general (red neuronal de $ L $ capas).  \n",
    "\n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2** </td>\n",
    "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b2** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Módulo de Propagación Hacia Adelante  \n",
    "\n",
    "### 4.1 - Propagación Lineal  \n",
    "\n",
    "Ahora que has inicializado tus parámetros, procederás con el módulo de propagación hacia adelante. Comenzarás implementando algunas funciones básicas que utilizarás más adelante para construir el modelo completo. Implementarás tres funciones en el siguiente orden:  \n",
    "\n",
    "- **LINEAR**  \n",
    "- **LINEAR -> ACTIVATION**, donde **ACTIVATION** será **ReLU** o **Sigmoide**.  \n",
    "- **[LINEAR -> RELU] $ \\times (L-1) $ -> LINEAR -> SIGMOIDE** (modelo completo).  \n",
    "\n",
    "El módulo de propagación lineal (vectorizado sobre todos los ejemplos) calcula la siguiente ecuación:  \n",
    "\n",
    "\n",
    "$$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} $$\n",
    "\n",
    "donde $ A^{[0]} = X $.  \n",
    "\n",
    "### **Ejercicio**: Construir la parte lineal de la propagación hacia adelante.  \n",
    "\n",
    "### **Recordatorio**:  \n",
    "La representación matemática de esta unidad es:  \n",
    "\n",
    "$$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "También puedes encontrar útil la función `np.dot()`. Si tus dimensiones no coinciden, imprimir `W.shape` puede ayudarte a depurar el error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td> **Z** </td>\n",
    "    <td> [[ 3.26295337 -1.23429987]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Propagación Hacia Adelante con Activación  \n",
    "\n",
    "En este cuaderno, utilizarás dos funciones de activación:  \n",
    "\n",
    "- **Sigmoide**:  \n",
    "  $$\n",
    "  \\sigma(Z) = \\sigma(W A + b) = \\frac{1}{1 + e^{-(W A + b)}}\n",
    "  $$\n",
    "  Te hemos proporcionado la función `sigmoid`. Esta función devuelve **dos** elementos:  \n",
    "  - El valor de activación \"`A`\".  \n",
    "  - Una \"`cache`\" que contiene \"`Z`\" (lo cual se usará en la función de propagación hacia atrás correspondiente).  \n",
    "\n",
    "  Para utilizarla, simplemente llama:  \n",
    "  ```python\n",
    "  A, activation_cache = sigmoid(Z)\n",
    "  ```\n",
    "\n",
    "- **ReLU**:  \n",
    "  La fórmula matemática de ReLU es:  \n",
    "  $$\n",
    "  A = RELU(Z) = \\max(0, Z)\n",
    "  $$\n",
    "  Te hemos proporcionado la función `relu`. Esta función también devuelve **dos** elementos:  \n",
    "  - El valor de activación \"`A`\".  \n",
    "  - Una \"`cache`\" que contiene \"`Z`\" (lo cual se usará en la función de propagación hacia atrás correspondiente).  \n",
    "\n",
    "  Para utilizarla, simplemente llama:  \n",
    "  ```python\n",
    "  A, activation_cache = relu(Z)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mayor comodidad, vas a agrupar dos funciones (**Lineal** y **Activación**) en una sola función (**LINEAR->ACTIVATION**). Por lo tanto, implementarás una función que realice el paso de propagación lineal seguido del paso de activación.  \n",
    "\n",
    "### **Ejercicio**:  \n",
    "Implementa la propagación hacia adelante de la capa *LINEAR->ACTIVATION*. La relación matemática es:  \n",
    "\n",
    "$$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} + b^{[l]})$$\n",
    "\n",
    "donde la función de activación **\"g\"** puede ser `sigmoid()` o `relu()`.  \n",
    "\n",
    "Usa `linear_forward()` y la función de activación correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "       \n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td> **With sigmoid: A ** </td>\n",
    "    <td > [[ 0.96890023  0.11013289]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **With ReLU: A ** </td>\n",
    "    <td > [[ 3.43896131  0.        ]]</td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d) Modelo de $L$ Capas**  \n",
    "\n",
    "Para hacer aún más conveniente la implementación de la Red Neuronal de $L$ capas, necesitarás una función que replique la función anterior (`linear_activation_forward` con **ReLU**) $L-1$ veces, y luego la siga con una llamada a `linear_activation_forward` con **Sigmoide**.  \n",
    "\n",
    "### **Ejercicio**:  \n",
    "Implementa la propagación hacia adelante del modelo mostrado arriba.  \n",
    "\n",
    "### **Instrucciones**:  \n",
    "En el código de abajo, la variable `AL` representará:  \n",
    "\n",
    "$$A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$$\n",
    "\n",
    "(Esta variable a veces también se denomina `Yhat`, es decir, $\\hat{Y} $).  \n",
    "\n",
    "### **Consejos**:  \n",
    "- Usa las funciones que ya has implementado.  \n",
    "- Utiliza un **bucle for** para replicar **[LINEAR->RELU] $ (L-1) $ veces**.  \n",
    "- No olvides almacenar los valores intermedios en la lista `\"caches\"`. Para agregar un nuevo valor `c` a una lista, puedes usar:  \n",
    "  ```python\n",
    "  list.append(c)\n",
    "  ```  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <td> **AL** </td>\n",
    "    <td > [[ 0.17007265  0.2524272 ]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Length of caches list ** </td>\n",
    "    <td > 2</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Genial! Ahora tienes una propagación hacia adelante completa que toma la entrada $ X $ y produce un vector fila $A^{[L]} $ que contiene tus predicciones. Además, almacena todos los valores intermedios en `\"caches\"`.  \n",
    "\n",
    "Usando $ A^{[L]} $, puedes calcular el costo de tus predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5 - Función de Costo**  \n",
    "\n",
    "Ahora implementarás la propagación hacia adelante y hacia atrás. Necesitas calcular el costo para verificar si tu modelo realmente está aprendiendo.  \n",
    "\n",
    "### **Ejercicio**:  \n",
    "Calcula la función de costo **entropía cruzada** $J$ usando la siguiente fórmula:  \n",
    "\n",
    "$$ J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\left( y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1 - a^{[L](i)}\\right) \\right) \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <td>**cost** </td>\n",
    "    <td> 0.41493159961539694</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6 - Módulo de Propagación Hacia Atrás**  \n",
    "\n",
    "Al igual que en la propagación hacia adelante, implementarás funciones auxiliares para la propagación hacia atrás (**backpropagation**). Recuerda que la propagación hacia atrás se utiliza para calcular el **gradiente de la función de pérdida con respecto a los parámetros**.  \n",
    "\n",
    "### **Recordatorio**  \n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">  \n",
    "<caption><center> **Figura 3** : Propagación hacia adelante y hacia atrás para *LINEAR->RELU->LINEAR->SIGMOID* <br> *Los bloques morados representan la propagación hacia adelante, y los bloques rojos representan la propagación hacia atrás.*  </center></caption>  \n",
    "\n",
    "---\n",
    "\n",
    "### **Construcción de la Propagación Hacia Atrás**  \n",
    "De manera similar a la propagación hacia adelante, construirás la propagación hacia atrás en **tres pasos**:  \n",
    "\n",
    "1. **Propagación LINEAR hacia atrás**  \n",
    "2. **Propagación LINEAR -> ACTIVATION hacia atrás**, donde ACTIVATION calculará la derivada de la función de activación **ReLU** o **sigmoide**.  \n",
    "3. **[LINEAR -> RELU] $ \\times (L-1) $ -> LINEAR -> SIGMOID hacia atrás** (modelo completo).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.1 - Propagación Lineal Hacia Atrás**  \n",
    "\n",
    "Para la capa $ l $, la parte lineal se define como:  \n",
    "\n",
    "$$\n",
    "Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}\n",
    "$$\n",
    "\n",
    "(seguida por una activación).  \n",
    "\n",
    "Supongamos que ya has calculado la derivada:  \n",
    "\n",
    "$$\n",
    "dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}\n",
    "$$\n",
    "\n",
    "Tu objetivo es obtener los gradientes $ (dW^{[l]}, db^{[l]}, dA^{[l-1]}) $.  \n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">  \n",
    "<caption><center> **Figura 4** </center></caption>  \n",
    "\n",
    "Los tres valores de salida $ (dW^{[l]}, db^{[l]}, dA^{[l-1]}) $ se calculan utilizando la entrada $ dZ^{[l]} $. Aquí están las fórmulas que necesitas:  \n",
    "\n",
    "$$\n",
    "dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}\n",
    "$$\n",
    "\n",
    "$$\n",
    "db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)} \\tag{9}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use the 3 formulas above to implement linear_backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td> **dA_prev** </td>\n",
    "    <td > [[ 0.51822968 -0.19517421]\n",
    " [-0.40506361  0.15255393]\n",
    " [ 2.37496825 -0.89445391]] </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "        <td> **dW** </td>\n",
    "        <td > [[-0.10076895  1.40685096  1.64992505]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td> **db** </td>\n",
    "        <td> [[ 0.50629448]] </td> \n",
    "    </tr> \n",
    "    \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2 - Propagación Hacia Atrás con Activación Lineal**  \n",
    "\n",
    "Ahora, crearás una función que combine dos funciones auxiliares:  \n",
    "- **`linear_backward`** (propagación hacia atrás de la parte lineal).  \n",
    "- **`linear_activation_backward`** (propagación hacia atrás de la activación).  \n",
    "\n",
    "### **Funciones proporcionadas**  \n",
    "Para ayudarte a implementar `linear_activation_backward`, se te han proporcionado dos funciones de propagación hacia atrás:  \n",
    "\n",
    "- **`sigmoid_backward`**: Implementa la propagación hacia atrás para la unidad **Sigmoide**. Puedes llamarla de la siguiente manera:  \n",
    "\n",
    "  ```python\n",
    "  dZ = sigmoid_backward(dA, activation_cache)\n",
    "  ```\n",
    "\n",
    "- **`relu_backward`**: Implementa la propagación hacia atrás para la unidad **ReLU**. Puedes llamarla así:  \n",
    "\n",
    "  ```python\n",
    "  dZ = relu_backward(dA, activation_cache)\n",
    "  ```\n",
    "\n",
    "Si $g(.) $ es la función de activación, `sigmoid_backward` y `relu_backward` calculan:  \n",
    "\n",
    "$$\n",
    "dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}\n",
    "$$\n",
    "\n",
    "### **Ejercicio**:  \n",
    "Implementa la propagación hacia atrás para la capa **LINEAR->ACTIVATION** utilizando las funciones anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada con sigmoid:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td >[[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]] </td> \n",
    "\n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.05729622]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada con relu**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td > [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]] </td> \n",
    "\n",
    "  </tr> \n",
    "   <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.20837892]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.3 - Propagación Hacia Atrás en un Modelo de $ L $ Capas**  \n",
    "\n",
    "Ahora implementarás la función de propagación hacia atrás para toda la red neuronal. Recuerda que cuando implementaste la función `L_model_forward`, en cada iteración almacenaste un **cache** que contiene los valores de **(X, W, b, y Z)**.  \n",
    "\n",
    "En el módulo de propagación hacia atrás, utilizarás estas variables almacenadas para calcular los gradientes. Por lo tanto, en la función `L_model_backward`, iterarás **hacia atrás a través de todas las capas ocultas**, comenzando desde la capa $ L $. En cada paso, usarás los valores almacenados para **retropropagar** a través de la capa correspondiente.  \n",
    "\n",
    "La **Figura 5** muestra el proceso de propagación hacia atrás:  \n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">  \n",
    "<caption><center>  **Figura 5** : Propagación hacia atrás  </center></caption>  \n",
    "\n",
    "---\n",
    "\n",
    "### **Inicialización de la Retropropagación**  \n",
    "\n",
    "Para retropropagar a través de la red, sabemos que la salida es:  \n",
    "\n",
    "$$\n",
    "A^{[L]} = \\sigma(Z^{[L]})\n",
    "$$\n",
    "\n",
    "Por lo tanto, tu código debe calcular la derivada del costo con respecto a $ A^{[L]} $, es decir:  \n",
    "\n",
    "$$\n",
    "dAL = \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}\n",
    "$$\n",
    "\n",
    "Puedes calcularlo con la siguiente fórmula (derivada del costo con respecto a $ A^{[L]} $):  \n",
    "\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Iteración Hacia Atrás por las Capas**  \n",
    "\n",
    "1. **Usa `dAL` en la función LINEAR->SIGMOID hacia atrás**  \n",
    "   - Esto usará los valores almacenados en el cache de `L_model_forward`.  \n",
    "2. **Utiliza un bucle `for` para iterar a través de todas las demás capas ocultas**  \n",
    "   - En cada capa $ l $, utiliza la función **LINEAR->RELU hacia atrás**.  \n",
    "   - Almacena cada `dA`, `dW`, y `db` en un **diccionario `grads`**.  \n",
    "\n",
    "Para almacenar los gradientes en el diccionario `grads`, usa esta fórmula:  \n",
    "\n",
    "$$\n",
    "grads[\"dW\" + str(l)] = dW^{[l]} \\tag{15}\n",
    "$$\n",
    "\n",
    "Por ejemplo, para $ l = 3 $, esto almacenará $ dW^{[3]} $ en `grads[\"dW3\"]`.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Ejercicio**:  \n",
    "Implementa la retropropagación para el modelo **[LINEAR->RELU] $ \\times (L-1) $ -> LINEAR -> SIGMOID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td > dW1 </td> \n",
    "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > db1 </td> \n",
    "           <td > [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]] </td> \n",
    "  </tr> \n",
    "  \n",
    "  <tr>\n",
    "  <td > dA1 </td> \n",
    "           <td > [[ 0.          0.52257901]\n",
    " [ 0.         -0.3269206 ]\n",
    " [ 0.         -0.32070404]\n",
    " [ 0.         -0.74079187]] </td> \n",
    "\n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.4 - Actualización de Parámetros**  \n",
    "\n",
    "En esta sección, actualizarás los parámetros del modelo utilizando el **descenso del gradiente**.\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "donde $\\alpha$ es la **tasa de aprendizaje**.  \n",
    "\n",
    "Después de calcular los parámetros actualizados, guárdalos en el diccionario `parameters`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio**: Implementa `update_parameters()` para actualizar los parámetros utilizando el **descenso del gradiente**.  \n",
    "\n",
    "### **Instrucciones**:  \n",
    "Actualiza los parámetros aplicando el descenso del gradiente a cada $ W^{[l]} $ y $ b^{[l]} $ para $ l = 1, 2, ..., L $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "    <tr>\n",
    "    <td > W1 </td> \n",
    "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > b1 </td> \n",
    "           <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > W2 </td> \n",
    "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > b2 </td> \n",
    "           <td > [[-0.84610769]] </td> \n",
    "  </tr> \n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
