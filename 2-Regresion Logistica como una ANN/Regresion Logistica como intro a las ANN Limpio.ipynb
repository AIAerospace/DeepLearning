{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística como introducción a las redes neuronales\n",
    "\n",
    "Vamos a construir un clasificador de regresión logística para reconocer gatos. Esta tarea te guiará a hacerlo con una mentalidad de red neuronal y, además, fortalecerá tus intuiciones sobre el aprendizaje profundo.\n",
    "\n",
    "**Instrucciones:**\n",
    "- No utilices bucles (for/while) en tu código, a menos que las instrucciones lo pidan explícitamente.\n",
    "\n",
    "**Aprenderás a:**\n",
    "- Construir la arquitectura general de un algoritmo de aprendizaje, que incluye:\n",
    "    - Inicialización de parámetros\n",
    "    - Cálculo de la función de costo y su gradiente\n",
    "    - Uso de un algoritmo de optimización (descenso de gradiente)\n",
    "- Reunir las tres funciones anteriores en una función principal del modelo, en el orden correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Paquetes ##\n",
    "\n",
    "Primero, ejecutemos la celda de abajo para importar todos los paquetes que necesitarás durante esta tarea.  \n",
    "- [numpy](www.numpy.org) es el paquete fundamental para la computación científica con Python.  \n",
    "- [h5py](http://www.h5py.org) es un paquete común para interactuar con un conjunto de datos almacenado en un archivo H5.  \n",
    "- [matplotlib](http://matplotlib.org) es una biblioteca famosa para graficar en Python.  \n",
    "- [PIL](http://www.pythonware.com/products/pil/) y [scipy](https://www.scipy.org/) se utilizan aquí para probar tu modelo con tu propia imagen al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - Visión general del conjunto de ejercicios ##\n",
    "\n",
    "**Enunciado del problema**: Se te proporciona un conjunto de datos (\"data.h5\") que contiene:  \n",
    "- un conjunto de entrenamiento de m_train imágenes etiquetadas como gato (y=1) o no gato (y=0)  \n",
    "- un conjunto de prueba de m_test imágenes etiquetadas como gato o no gato  \n",
    "- cada imagen tiene forma (num_px, num_px, 3), donde 3 corresponde a los 3 canales (RGB). Por lo tanto, cada imagen es cuadrada (altura = num_px, ancho = num_px).  \n",
    "\n",
    "Construirás un algoritmo sencillo de reconocimiento de imágenes que pueda clasificar correctamente las imágenes como gato o no gato.\n",
    "\n",
    "Familiaricémonos más con el conjunto de datos. Carga los datos ejecutando el siguiente código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cargar el dataset (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregamos \"_orig\" al final de los conjuntos de datos de imágenes (train y test) porque los vamos a preprocesar. Después del preprocesamiento, obtendremos train_set_x y test_set_x (las etiquetas train_set_y y test_set_y no necesitan ningún preprocesamiento).\n",
    "\n",
    "Cada línea de tus train_set_x_orig y test_set_x_orig es un array que representa una imagen. Puedes visualizar un ejemplo ejecutando el siguiente código. Puedes cambiar el valor de `index` y volver a ejecutar para ver otras imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dibujamos un caso del dataset\n",
    "index = 25\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", \" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchos errores provienen de dimensiones de matrices/vectores que no coinciden. Si logras mantener las dimensiones de matrices/vectores en orden, avanzarás mucho en la eliminación de muchos errores.\n",
    "\n",
    "**Ejercicio:** Encuentra los valores de:\n",
    "- `m_train` (número de ejemplos de entrenamiento)\n",
    "- `m_test` (número de ejemplos de prueba)\n",
    "- `num_px` (= altura = ancho de una imagen de entrenamiento)\n",
    "\n",
    "Recuerda que `train_set_x_orig` es un array de numpy con la forma `(m_train, num_px, num_px, 3)`. Por ejemplo, puedes acceder a `m_train` escribiendo `train_set_x_orig.shape[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada para `m_train`, `m_test` y `num_px`**:  \n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td>**m_train**</td>\n",
    "    <td> 209 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**m_test**</td>\n",
    "    <td> 50 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**num_px**</td>\n",
    "    <td> 64 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mayor comodidad, ahora deberías reestructurar las imágenes con forma `(num_px, num_px, 3)` en un array de numpy con forma `(num_px * num_px * 3, 1)`. Después de esto, nuestro conjunto de datos de entrenamiento (y prueba) será un array de numpy donde cada columna representa una imagen aplanada. Debe haber `m_train` (o respectivamente `m_test`) columnas.\n",
    "\n",
    "**Ejercicio:** Reestructura los conjuntos de datos de entrenamiento y prueba de modo que las imágenes de tamaño `(num_px, num_px, 3)` se aplanen en vectores individuales con forma `(num_px * num_px * 3, 1)`.\n",
    "\n",
    "Un truco cuando deseas aplanar una matriz `X` con forma `(a, b, c, d)` en una matriz `X_flatten` con forma `(b * c * d, a)` es usar:  \n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T es la transpuesta de X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reshape el dataset de entrenamiento y test\n",
    "\n",
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:  \n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td>**train_set_x_flatten shape**</td>\n",
    "    <td>(12288, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**train_set_y shape**</td>\n",
    "    <td>(1, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**test_set_x_flatten shape**</td>\n",
    "    <td>(12288, 50)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**test_set_y shape**</td>\n",
    "    <td>(1, 50)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**verificación de coherencia después de reestructurar**</td>\n",
    "    <td>[17 31 56 22 33]</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para representar imágenes a color, se deben especificar los canales rojo, verde y azul (RGB) para cada píxel. Por lo tanto, el valor de un píxel es, en realidad, un vector de tres números que van de 0 a 255.\n",
    "\n",
    "Un paso común de preprocesamiento en el aprendizaje automático es centrar y estandarizar tu conjunto de datos, lo que significa restar la media de todo el array de numpy a cada ejemplo y luego dividir cada ejemplo por la desviación estándar de todo el array de numpy. Sin embargo, para conjuntos de datos de imágenes, es más simple, conveniente y casi igual de efectivo simplemente dividir cada fila del conjunto de datos por 255 (el valor máximo de un canal de píxeles).\n",
    "\n",
    "Estandaricemos nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\alpha}{\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Algoritmo ##\n",
    "\n",
    "**Expresión matemática del algoritmo**:\n",
    "\n",
    "Para un ejemplo $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "El coste se calcula luego sumando sobre todos los ejemplos de entrenamiento:  \n",
    "$$J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)}) \\tag{6}$$\n",
    "\n",
    "\n",
    "\n",
    "**Pasos clave**:  \n",
    "En este ejercicio, llevarás a cabo los siguientes pasos:  \n",
    "- Inicializar los parámetros del modelo.  \n",
    "- Aprender los parámetros del modelo minimizando el costo.  \n",
    "- Usar los parámetros aprendidos para hacer predicciones (en el conjunto de prueba).  \n",
    "- Analizar los resultados y sacar conclusiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Construyendo las partes de nuestro algoritmo ##\n",
    "\n",
    "Los pasos principales para construir una Red Neuronal son:  \n",
    "1. Definir la estructura del modelo (como el número de características de entrada).  \n",
    "2. Inicializar los parámetros del modelo.  \n",
    "3. Bucle:  \n",
    "    - Calcular la pérdida actual (propagación hacia adelante).  \n",
    "    - Calcular el gradiente actual (propagación hacia atrás).  \n",
    "    - Actualizar los parámetros (descenso de gradiente).  \n",
    "\n",
    "A menudo, los pasos 1-3 se construyen por separado y luego se integran en una función que llamamos `model()`.\n",
    "\n",
    "### 4.1 - Funciones auxiliares\n",
    "\n",
    "implementa `sigmoid()`:  \n",
    "$$sigmoid(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$\n",
    "Utiliza `np.exp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>**sigmoid([0, 2])**</td>\n",
    "    <td> [ 0.5         0.88079708]</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Inicialización de parámetros\n",
    "\n",
    "**Ejercicio:** Implementa la inicialización de parámetros en la celda a continuación. Debes inicializar \\( w \\) como un vector de ceros. Si no sabes qué función de numpy usar, consulta la documentación de la biblioteca de Numpy sobre `np.zeros()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim = 4\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "\n",
    "<table style=\"width:15%\">\n",
    "    <tr>\n",
    "        <td>  ** w **  </td>\n",
    "        <td> [[ 0.]\n",
    " [ 0.]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** b **  </td>\n",
    "        <td> 0 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "For image inputs, w will be of shape (num_px $\\times$ num_px $\\times$ 3, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Propagación hacia adelante y hacia atrás\n",
    "\n",
    "Ahora que tus parámetros están inicializados, puedes realizar los pasos de \"propagación hacia adelante\" y \"propagación hacia atrás\" para aprender los parámetros.\n",
    "\n",
    "**Ejercicio:** Implementa una función `propagate()` que calcule la función de costo y su gradiente.\n",
    "\n",
    "**Pistas**:\n",
    "\n",
    "Propagación hacia adelante:\n",
    "- Tienes X\n",
    "- Calculas $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Funcion de costes: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Propagación hacia atras:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "                                     # compute activation\n",
    "                                     # compute cost\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w, b, X, Y = np.array([[1],[2]]), 2, np.array([[1,2],[3,4]]), np.array([[1,0]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "    <tr>\n",
    "        <td>  ** dw **  </td>\n",
    "        <td> [[ 0.99993216]\n",
    " [ 1.99980262]]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** db **  </td>\n",
    "        <td> 0.499935230625 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** cost **  </td>\n",
    "        <td> 6.000064773192205</td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Optimización\n",
    "\n",
    "- Has inicializado tus parámetros.  \n",
    "- También eres capaz de calcular una función de costo y su gradiente.  \n",
    "- Ahora, deseas actualizar los parámetros utilizando el descenso de gradiente.  \n",
    "\n",
    "**Ejercicio:** Escribe la función de optimización. El objetivo es aprender \\( w \\) y \\( b \\) minimizando la función de costo \\( J \\). Para un parámetro $\\theta $, la regla de actualización es: \n",
    " $$\\theta = \\theta - \\alpha \\, d\\theta$$\n",
    "donde $\\alpha$ es la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "    <tr>\n",
    "       <td> **w** </td>\n",
    "       <td>[[ 0.1124579 ]\n",
    " [ 0.23106775]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> **b** </td>\n",
    "       <td> 1.55930492484 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> **dw** </td>\n",
    "       <td> [[ 0.90158428]\n",
    " [ 1.76250842]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> **db** </td>\n",
    "       <td> 0.430462071679 </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** La función anterior proporcionará los valores aprendidos de w  y b . Ahora podemos usar w  y b  para predecir las etiquetas de un conjunto de datos  X . Implementa la función `predict()`. Hay dos pasos para calcular las predicciones:\n",
    "\n",
    "1. Calcula  \n",
    "   $$ \\hat{Y} = A = \\sigma(w^T X + b) $$\n",
    "\n",
    "2. Convierte las entradas de A en 0 (si la activación  $\\leq 0.5$) o 1 (si la activación > 0.5), y almacena las predicciones en un vector `Y_prediction`. Si lo prefieres, puedes usar una declaración `if`/`else` dentro de un bucle `for` (aunque también hay una forma de vectorizar esto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m), dtype=int)\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"predictions = \" + str(predict(w, b, X)))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:30%\">\n",
    "    <tr>\n",
    "         <td>\n",
    "             **predictions**\n",
    "         </td>\n",
    "          <td>\n",
    "            [[ 1  1]]\n",
    "         </td>  \n",
    "   </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Unir todas las funciones en un modelo ##\n",
    "\n",
    "Ahora verás cómo se estructura el modelo general combinando todos los bloques de construcción (funciones implementadas en las partes anteriores) en el orden correcto.\n",
    "\n",
    "**Ejercicio:** Implementa la función del modelo. Usa la siguiente notación:\n",
    "- `Y_prediction` para tus predicciones en el conjunto de prueba.  \n",
    "- `Y_prediction_train` para tus predicciones en el conjunto de entrenamiento.  \n",
    "- `w`, `costs`, `grads` para las salidas de `optimize()`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    \n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    \n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    \n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:40%\"> \n",
    "    <tr>\n",
    "        <td> **Train Accuracy**  </td> \n",
    "        <td> 99.04306220095694 % </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>**Test Accuracy** </td> \n",
    "        <td> 70.0 % </td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Comentario**: La precisión en el entrenamiento está cerca del 100%. Esto es una buena verificación de coherencia: tu modelo está funcionando y tiene la capacidad suficiente para ajustarse a los datos de entrenamiento. El error en el conjunto de prueba es del 68%. En realidad, no está mal para este modelo simple, dado el pequeño conjunto de datos que usamos y que la regresión logística es un clasificador lineal. \n",
    "\n",
    "Hay que recordar que hay que comparar esta precisión con la de un clasificador trivial, uno que contestara completamente al azar, y acertaría un 50%. Es decir, hemos mejorado este 50% hasta un 68%.\n",
    "\n",
    "Además, puedes ver que el modelo está claramente sobreajustando los datos de entrenamiento. Más adelante en esta especialización, aprenderás cómo reducir el sobreajuste, por ejemplo, utilizando regularización. \n",
    "\n",
    "Usando el código a continuación (y cambiando la variable `index`), puedes observar las predicciones en imágenes del conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ejemplos\n",
    "index = 5\n",
    "plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\n",
    "print (\"y = \" + str(test_set_y[0,index]), \"prediccion = \" + classes[d[\"Y_prediction_test\"][0,index]].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dibujamos la curva de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Interpretación**:  \n",
    "Puedes observar que el coste está disminuyendo, lo cual indica que los parámetros están siendo aprendidos. Sin embargo, también puedes notar que podrías entrenar el modelo aún más en el conjunto de entrenamiento. Intenta aumentar el número de iteraciones en la celda anterior y vuelve a ejecutar las celdas. Es posible que veas que la precisión en el conjunto de entrenamiento aumenta, pero la precisión en el conjunto de prueba disminuye. Esto se llama sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Cambiar learning rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elección de la tasa de aprendizaje ####\n",
    "\n",
    "**Recordatorio**:  \n",
    "Para que el Descenso de Gradiente funcione, debes elegir la tasa de aprendizaje \\( \\alpha \\) de manera adecuada. La tasa de aprendizaje determina cómo de rápido actualizamos los parámetros. Si la tasa de aprendizaje es demasiado grande, podemos \"sobrepasar\" el valor óptimo. De manera similar, si es demasiado pequeña, necesitaremos demasiadas iteraciones para converger a los mejores valores. Por eso es crucial utilizar una tasa de aprendizaje bien ajustada.\n",
    "\n",
    "Comparemos la curva de aprendizaje de nuestro modelo con varias elecciones de tasas de aprendizaje. Ejecuta la celda a continuación. Esto debería tomar aproximadamente 1 minuto. Puedes probar valores diferentes a los tres que hemos inicializado en la variable `learning_rates` y observa qué sucede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print (\"learning rate is: \" + str(i))\n",
    "    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretación**:  \n",
    "- Diferentes tasas de aprendizaje producen diferentes costes y, por lo tanto, diferentes resultados en las predicciones.  \n",
    "- Si la tasa de aprendizaje es demasiado grande (por ejemplo, 0.01), el coste puede oscilar hacia arriba y hacia abajo. Incluso puede diverger (aunque en este ejemplo, usar 0.01 eventualmente conduce a un buen valor para el coste).  \n",
    "- Un coste más bajo no significa necesariamente un mejor modelo. Debes verificar si existe un posible sobreajuste. Esto ocurre cuando la precisión en el entrenamiento es mucho mayor que la precisión en el conjunto de prueba.  \n",
    "- En el aprendizaje profundo, generalmente recomendamos que:  \n",
    "  - Elijas la tasa de aprendizaje que mejor minimice la función de costes.  \n",
    "  - Si tu modelo presenta sobreajuste, utilices otras técnicas para reducirlo.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
